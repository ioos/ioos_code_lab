{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIX-_9o2P07V",
    "outputId": "09d2de4a-1f34-4518-c51f-487e14b55b66",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def _install(package):\n",
    "    if COLAB:\n",
    "        ans = input(f\"Install { package }? [y/n]:\")\n",
    "        if ans.lower() in [\"y\", \"yes\"]:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package]\n",
    "            )\n",
    "            print(f\"{ package } installed!\")\n",
    "\n",
    "\n",
    "def _colab_install_missing_deps(deps):\n",
    "    import importlib\n",
    "\n",
    "    for dep in deps:\n",
    "        if importlib.util.find_spec(dep) is None:\n",
    "            if dep == \"iris\":\n",
    "                dep = \"scitools-iris\"\n",
    "            _install(dep)\n",
    "\n",
    "\n",
    "deps = [\"ckanapi\", \"geopandas\"]\n",
    "\n",
    "\n",
    "_colab_install_missing_deps(deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatically query the IOOS Data Catalog for a specific observation type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created: 2024-09-17\n",
    "\n",
    "Updated: 2025-03-06\n",
    "\n",
    "Author: [Mathew Biddle](mailto:mathew.biddle@noaa.gov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl6UQcydrdtx"
   },
   "source": [
    "In this notebook we highlight the ability to search the [IOOS Data Catalog](https://data.ioos.us/) for a specific subset of observations using the [CKAN](https://ckan.org/) web accessible Application Programming Interface (API). \n",
    "\n",
    "For this example, we want to look for observations of oxygen in the water column across the IOOS Catalog. As part of the [IOOS Metadata Profile](https://ioos.github.io/ioos-metadata/), which the US IOOS community uses to publish datasets, we know that each Regional Association and DAC will be following the [Climate and Forecast (CF) Conventions](http://cfconventions.org/) and using CF `standard_names` to describe their datasets. So, with that assumption, we can search across the IOOS Data catalog for datasets with the CF standard names that contain `oxygen` and `sea_water`. Then, we can build a simple map to show the geographical distribution of those datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz_XVNHerUus"
   },
   "source": [
    "## Build CKAN API query base.\n",
    "\n",
    "Uses https://github.com/ckan/ckanapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ilaNW-tPtVy",
    "outputId": "9bf22340-1404-48e6-b3e8-fdc2cd6a3d23"
   },
   "outputs": [],
   "source": [
    "from ckanapi import RemoteCKAN\n",
    "\n",
    "\n",
    "ioos_catalog = RemoteCKAN(\n",
    "    address=\"https://data.ioos.us\",\n",
    "    user_agent=\"ckanapiioos/1.0 (+https://ioos.us/)\",\n",
    ")\n",
    "\n",
    "\n",
    "ioos_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DISgdWPrRd0"
   },
   "source": [
    "## What organizations are in the catalog?\n",
    "\n",
    "Tell me what organizations are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4joF0z8Px-m",
    "outputId": "95a91429-b9aa-4350-b67c-6d0f07e4a12c"
   },
   "outputs": [],
   "source": [
    "orgs = ioos_catalog.action.organization_list()\n",
    "print(orgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN2ELDQZrNah"
   },
   "source": [
    "## How many datasets are we searching across?\n",
    "\n",
    "Grab all the datasets available and return the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ov9sSwpP8VP",
    "outputId": "4d06f0ca-0ad6-4260-ecd5-52f8019652be"
   },
   "outputs": [],
   "source": [
    "datasets = ioos_catalog.action.package_search()\n",
    "datasets[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkgC5oLfmGB1"
   },
   "source": [
    "## Grab the most recent applicable CF standard names\n",
    "\n",
    "Collect [CF standard names](https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html) that contain `oxygen` and `sea_water` from the CF standard name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "enKjucgnXivM",
    "outputId": "9d2c266f-755a-42dc-9b5e-40607552c56f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://cfconventions.org/Data/cf-standard-names/current/src/cf-standard-name-table.xml\"\n",
    "\n",
    "tbl_version = pd.read_xml(url, xpath=\"./*\")[\"version_number\"][0].astype(int)\n",
    "df = pd.read_xml(url, xpath=\"entry\")\n",
    "\n",
    "std_names = df.loc[\n",
    "    (df[\"id\"].str.contains(\"oxygen\") & df[\"id\"].str.contains(\"sea_water\"))\n",
    "]\n",
    "\n",
    "print(f\"CF Standard Name Table: {tbl_version}\")\n",
    "\n",
    "std_names[[\"id\", \"description\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-KxN2RlpLOu"
   },
   "source": [
    "## Search across IOOS Data Catalog using CKAN API\n",
    "\n",
    "Search the IOOS Data Catalog for CF standard names that match those above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from ckanapi import RemoteCKAN\n",
    "from ckanapi.errors import CKANAPIError\n",
    "\n",
    "from urllib3.exceptions import IncompleteRead\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "\n",
    "\n",
    "ua = \"ckanapiioos/1.0 (+https://ioos.us/)\"\n",
    "\n",
    "ioos_catalog = RemoteCKAN(\"https://data.ioos.us\", user_agent=ua)\n",
    "ioos_catalog\n",
    "\n",
    "df_out = pd.DataFrame()\n",
    "\n",
    "for std_name in std_names[\"id\"]:\n",
    "\n",
    "    print(std_name)\n",
    "\n",
    "    fq = f\"+cf_standard_names:{std_name}\"\n",
    "\n",
    "    result_count = 0\n",
    "\n",
    "    df_std_name = pd.DataFrame()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            datasets = ioos_catalog.action.package_search(fq=fq, rows=500, start=result_count)\n",
    "        except (CKANAPIError, IncompleteRead, ChunkedEncodingError):\n",
    "            continue\n",
    "\n",
    "        num_results = datasets[\"count\"]\n",
    "\n",
    "        print(f\"num_results: {num_results}, result_count: {result_count}\")\n",
    "\n",
    "        for dataset in datasets[\"results\"]:\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"title\": [dataset[\"title\"]],\n",
    "                    \"url\": [dataset[\"resources\"][0][\"url\"]],\n",
    "                    \"org\": [dataset[\"organization\"][\"title\"]],\n",
    "                    \"std_name\": std_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            df_std_name = pd.concat([df_std_name, df], ignore_index=True)\n",
    "            result_count = df_std_name.shape[0]\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        if result_count >= num_results:\n",
    "            print(f\"num_results: {num_results}, result_count: {result_count}\")\n",
    "            break\n",
    "            \n",
    "    df_out = pd.concat([df_out, df_std_name], ignore_index=True)\n",
    "    \n",
    "    print(f\"num_results: {num_results}, result_count: {result_count}, total_result_count: {df_out.shape[0]}\")\n",
    "    \n",
    "df_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0atXi0kEnbr"
   },
   "source": [
    "## Do some summarizing of the responses\n",
    "\n",
    "The DataFrame of the matching datasets is quite large. I wonder what the distribution of those datasets across organizations looks like? Let's use [pandas.groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to generate some statistics about how many datasets are provided, matching our criteria, by which organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "K3tcW2iyDpFd",
    "outputId": "aacb7d85-9c9c-4fe8-ba2e-74d805a7deb5"
   },
   "outputs": [],
   "source": [
    "df_out.groupby(by=\"org\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9PwQW_7Gj0a"
   },
   "source": [
    "## Drop the Glider DAC data\n",
    "\n",
    "Glider DAC data are already making it to NCEI, so we can drop those entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "u0y8KMRYGhl9",
    "outputId": "21f54f00-02e1-497b-a45e-83ee0004ca4b"
   },
   "outputs": [],
   "source": [
    "df_out_no_glider = df_out.loc[~df_out[\"org\"].str.contains(\"Glider DAC\")]\n",
    "df_out_no_glider.groupby(by=\"org\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVz8vdNkEt6X"
   },
   "source": [
    "## Digging into some of the nuances\n",
    "\n",
    "There are still quite a lot of datasets from each organization. As our search above looked for each CF standard_name across all the datasets, there might be duplicate datasets which have multiple matching CF standard names. ie. one dataset might have both `mass_concentration_of_oxygen_in_sea_water` and `fractional_saturation_of_oxygen_in_sea_water`, but we only need to know that it's one dataset.\n",
    "\n",
    "As we only need to know about the unique datasets, let's count how many unique dataset urls we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "0wygCC8X5Tpc",
    "outputId": "48be015f-7e8d-45af-b2cd-e7a37fa6bf86"
   },
   "outputs": [],
   "source": [
    "df_out_no_glider.groupby(by=\"url\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate records\n",
    "\n",
    "As you can see above, there are a lot of duplicate dataset urls which we can simplify down. We identify duplicates by looking at the URL, which should be unique for each dataset, and drop the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "H6rQj51d7cAm",
    "outputId": "0942f042-834a-415c-e276-ad0e89e732ed"
   },
   "outputs": [],
   "source": [
    "df_out_nodups_no_glider = df_out_no_glider.drop_duplicates(subset=[\"url\"], keep=\"last\")\n",
    "\n",
    "df_out_nodups_no_glider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDsBe8k9E-47"
   },
   "source": [
    "## How many endpoints are not ERDDAP?\n",
    "\n",
    "Now we have a unique list of datasets which match our CF standard name criteria. Since we have some background in using [ERDDAP to query for data](https://ioos.github.io/ioos_code_lab/content/code_gallery/data_access_notebooks/2017-03-21-ERDDAP_IOOS_Sensor_Map.html), let's take a look at what other endpoints each of the datasets are using.\n",
    "\n",
    "_Hint: We know ERDDAP systems typically have `erddap` in their urls._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "saDpOVP5778u",
    "outputId": "f0a8f4bb-e77a-4020-b295-f66cfd57fb64"
   },
   "outputs": [],
   "source": [
    "df_out_nodups_no_glider.loc[~df_out_nodups_no_glider[\"url\"].str.contains(\"erddap\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_PGG_wsHYwF"
   },
   "source": [
    "## What's the remaining distribution?\n",
    "\n",
    "This is the distribution of unique datasets found in the IOOS Data Catalog which have a CF Standard Name that contains the work `oxygen` and `sea_water`. We've dropped out the Glider DAC datasets as, theoretically, those are in NCEI already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "fRHL-7lPGwOL",
    "outputId": "b72e8a38-1e63-44b1-9644-5eaa7df9abc6"
   },
   "outputs": [],
   "source": [
    "df_out_nodups_no_glider.groupby(by=\"org\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data\n",
    "\n",
    "Let's rip through all of the datasets, grab the data as a table (including units) and make a monster dictionary. This takes a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk9myBdgBUxH"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import joblib\n",
    "import stamina\n",
    "\n",
    "\n",
    "@stamina.retry(on=HTTPError, attempts=3)\n",
    "def request_df(url):\n",
    "    \"\"\"Thin layer to handle retries.\"\"\"\n",
    "    return pd.read_csv(url, low_memory=False)\n",
    "\n",
    "\n",
    "def error_handling_layer(row):\n",
    "    \"\"\"Even with stamina we may hit servers that will fail.\"\"\"\n",
    "    title = row[\"title\"]\n",
    "     # Requesting only the position.\n",
    "    url = f\"{row['url']}.csvp?latitude,longitude&distinct()\"\n",
    "    try:\n",
    "        df = request_df(url)\n",
    "    except Exception as err:\n",
    "        msg = f\"Failed to fetch {url}. {err}.\"\n",
    "        print(msg)\n",
    "        df = None\n",
    "    return title, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_iter = len(df_out_nodups_no_glider)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "downloads = [\n",
    "    r for r in tqdm(\n",
    "        joblib.Parallel(return_as=\"generator\", n_jobs=num_cores, max_nbytes=5000)(\n",
    "            joblib.delayed(error_handling_layer)(row) for _, row in df_out_nodups_no_glider.iterrows()\n",
    "        ), total=n_iter)\n",
    "]\n",
    "\n",
    "\n",
    "dict_out_final = dict(downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at one of the DataFrames.\n",
    "\n",
    "Transpose it when we print, so we can see all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_out_final[\n",
    "    '\"Deepwater CTD - pe972218.ctd.nc - 29.25N, -87.89W - 1997-03-21\"'\n",
    "].head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a nice map of the distribution of observations\n",
    "\n",
    "Below we create a mapping function to plot the unique dataset points on a map. Then, we use that function with our full response. We have to do a little reorganizing of the data to build one DataFrame for all the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.io.shapereader as shpreader\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_map(df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shpfilename = shpreader.natural_earth(\n",
    "        resolution=\"110m\",\n",
    "        category=\"cultural\",\n",
    "        name=\"admin_0_countries\",\n",
    "    )\n",
    "    countries = gpd.read_file(shpfilename)\n",
    "\n",
    "    countries[countries[\"NAME\"] == \"United States of America\"].plot(\n",
    "        color=\"lightgrey\", ax=ax\n",
    "    )\n",
    "\n",
    "    df.plot(\n",
    "        x=\"longitude (degrees_east)\",\n",
    "        y=\"latitude (degrees_north)\",\n",
    "        kind=\"scatter\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.grid(visible=True, alpha=0.5)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coords = pd.DataFrame(\n",
    "    columns=[\"latitude (degrees_north)\", \"longitude (degrees_east)\"]\n",
    ")\n",
    "\n",
    "for key in dict_out_final.keys():\n",
    "    df_coords = pd.concat(\n",
    "        [\n",
    "            df_coords,\n",
    "            dict_out_final[key][\n",
    "                [\"latitude (degrees_north)\", \"longitude (degrees_east)\"]\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# drop all duplicates\n",
    "df_coords_clean = df_coords.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# make the map\n",
    "make_map(df_coords_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets explore those points on an interactive map\n",
    "\n",
    "Just for fun, we can us [`geopandas.explore()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html) to plot these points on an interactive map to browse around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    df_coords_clean,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        df_coords_clean[\"longitude (degrees_east)\"],\n",
    "        df_coords_clean[\"latitude (degrees_north)\"],\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this example demonstrates the flexibility of direct requests to the IOOS Data Catalog CKAN server and all the possibilities it provides. In this notebook we:\n",
    "\n",
    "* Search the IOOS Data Catalog CKAN API with keywords.\n",
    "* Found datasets matching our specified criteria.\n",
    "* Collected all the data from each of the datasets matching our criteria.\n",
    "* Created a simple map of the distribution of datasets which match our criteria.\n",
    "\n",
    "To take this one step further, since we collected all the data from each of the datasets (in the dictionary `dict_out_final`) a user could integrate all of the oxygen observations together and start to build a comprehensive dataset. \n",
    "\n",
    "Additionally, a user could modify the CKAN query to search for terms outside of the CF standard names to potentially gather more datasets. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "authorship_tag": "ABX9TyOq6Zm4CP25L4Z2jB+P61RB",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
